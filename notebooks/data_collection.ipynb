{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection for Sentiment Analysis\n",
    "\n",
    "This notebook is dedicated to collecting raw social media data for sentiment analysis. We will utilize APIs or web scraping techniques to gather the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import os\n",
    "\n",
    "# Define absolute path for data directory\n",
    "directory = r\"C:\\Users\\ankit\\Desktop\\LORU\\sentiment-analysis-app\\data\\raw\"\n",
    "\n",
    "\n",
    "# Define absolute path for data file\n",
    "\n",
    "file_path = os.path.join(directory, \"social_media_data.csv\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "def save_to_csv(df, file_path):\n",
    "    if not df.empty:  # Ensure there is data to save\n",
    "        df.to_csv(file_path, index=False, mode=\"a\", header=not os.path.exists(file_path))\n",
    "        print(f\"Appended {len(df)} rows to {file_path}\")\n",
    "    else:\n",
    "        print(\"No data to append.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping subreddit: Happy\n",
      "Appended 500 rows to C:\\Users\\ankit\\Desktop\\LORU\\sentiment-analysis-app\\data\\raw\\social_media_data.csv\n",
      "Scraping and saving completed.\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Function to scrape posts from a subreddit\n",
    "def scrape_reddit(subreddit, max_posts=500):\n",
    "    base_url = f\"https://www.reddit.com/r/{subreddit}/new/.rss\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    \n",
    "    posts = []\n",
    "    \n",
    "    while len(posts) < max_posts:\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, \"xml\")\n",
    "        entries = soup.find_all(\"entry\")\n",
    "\n",
    "        for entry in entries:\n",
    "            if len(posts) >= max_posts:\n",
    "                break\n",
    "            posts.append({\n",
    "                \"title\": entry.title.text,\n",
    "                \"author\": entry.author.find(\"name\").text if entry.author else \"Unknown\",\n",
    "                \"link\": entry.link[\"href\"],\n",
    "                \"published\": entry.published.text,\n",
    "                \"content\": entry.content.text if entry.content else \"\"\n",
    "            })\n",
    "\n",
    "        time.sleep(1)  # Avoid too many requests in a short time\n",
    "    \n",
    "    return pd.DataFrame(posts)\n",
    "\n",
    "# List of subreddits to scrape\n",
    "\n",
    "subreddits = [\n",
    "            #     \"politics\",\n",
    "            #    \"technology\",\n",
    "            #      \"science\",\n",
    "            #        \"WutheringWaves\",\n",
    "            #          \"Brawlstars\",\n",
    "                       \"Happy\",\n",
    "                        #  \"UpliftingNews\",\n",
    "                        #    \"Depression\",\n",
    "                        #      \"tifu\",\n",
    "                        #        \"OffMyChest\",\n",
    "                        #          \"TodayILearned\"\n",
    "                                 ]\n",
    "\n",
    "# DataFrame to store all posts\n",
    "all_posts = pd.DataFrame()\n",
    "\n",
    "# Loop through each subreddit and scrape posts\n",
    "for subreddit in subreddits:\n",
    "    print(f\"Scraping subreddit: {subreddit}\")\n",
    "    df = scrape_reddit(subreddit, 500)  # Ensure scrape_reddit returns a DataFrame\n",
    "    save_to_csv(df, file_path)\n",
    "\n",
    "print(\"Scraping and saving completed.\")\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "print(all_posts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Exploration\n",
    "\n",
    "In this section, we will perform some initial exploration of the collected data to understand its structure and contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   title      500 non-null    object\n",
      " 1   author     500 non-null    object\n",
      " 2   link       500 non-null    object\n",
      " 3   published  500 non-null    object\n",
      " 4   content    500 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 19.7+ KB\n",
      "None\n",
      "                                                    title     author  \\\n",
      "count                                                 500        500   \n",
      "unique                                                 25         21   \n",
      "top     I got to go home recently and spend time with ...  /u/tacozy   \n",
      "freq                                                   20         80   \n",
      "\n",
      "                                                     link  \\\n",
      "count                                                 500   \n",
      "unique                                                 25   \n",
      "top     https://www.reddit.com/r/happy/comments/1j1f7j...   \n",
      "freq                                                   20   \n",
      "\n",
      "                        published  \\\n",
      "count                         500   \n",
      "unique                         25   \n",
      "top     2025-03-02T01:22:05+00:00   \n",
      "freq                           20   \n",
      "\n",
      "                                                  content  \n",
      "count                                                 500  \n",
      "unique                                                 25  \n",
      "top     <table> <tr><td> <a href=\"https://www.reddit.c...  \n",
      "freq                                                   20  \n"
     ]
    }
   ],
   "source": [
    "# Display basic information about the DataFrame\n",
    "if 'df' in locals():\n",
    "    print(df.info())\n",
    "    print(df.describe())\n",
    "else:\n",
    "    print('DataFrame not available for exploration.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Collected Data\n",
    "\n",
    "Finally, we will save the collected data to the raw data directory for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Define absolute path for data directory\n",
    "# directory = r\"C:\\Users\\ankit\\Desktop\\HYperion\\sentiment-analysis-app\\data\\raw\"\n",
    "# file_path = os.path.join(directory, \"social_media_data.csv\")\n",
    "\n",
    "# # Ensure the directory exists\n",
    "# os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# # Save the DataFrame if it exists\n",
    "# if \"df\" in locals():\n",
    "#     df.to_csv(file_path, index=False, mode=\"w\")\n",
    "#     print(f\"Data saved at: {file_path}\")\n",
    "# else:\n",
    "#     print(\"No data to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def save_to_csv(df, file_path):\n",
    "#     if not df.empty:  # Ensure there is data to save\n",
    "#         df.to_csv(file_path, index=False, mode=\"a\", header=not os.path.exists(file_path))\n",
    "#         print(f\"Appended {len(df)} rows to {file_path}\")\n",
    "#     else:\n",
    "#         print(\"No data to append.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
